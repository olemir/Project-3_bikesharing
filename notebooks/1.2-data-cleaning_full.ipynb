{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7445ead0",
   "metadata": {},
   "source": [
    "# Data Cleaning - full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8326833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyproj\\network.py:59: UserWarning: pyproj unable to set PROJ database path.\n",
      "  _set_context_ca_bundle_path(ca_bundle_path)\n"
     ]
    }
   ],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e467d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e2a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_station_data, merge_values_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa62ac0",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d181dc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\olive\\\\anaconda3\\\\envs\\\\base_data_analysis\\\\Library\\\\share\\\\proj'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyproj\n",
    "pyproj.datadir.get_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f19483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyogrio\\core.py:36: RuntimeWarning: Could not detect PROJ data files. Set PROJ_LIB environment variable to the correct path.\n",
      "  _init_proj_data()\n"
     ]
    },
    {
     "ename": "CRSError",
     "evalue": "Invalid projection: EPSG:4326: (Internal Proj Error: proj_create: no database context specified)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCRSError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# city boundaries of washington\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m washington_boundary = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw_data_paths\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwashington_outline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m washington_boundary = washington_boundary.drop(columns=[\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAREAMILES\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOBJECTID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSTATE_CITY\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCAPITAL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWEB_URL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGLOBALID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCREATOR\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCREATED\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEDITOR\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mEDITED\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSHAPEAREA\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSHAPELEN\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m     ])\n\u001b[32m      6\u001b[39m washington_boundary.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\io\\file.py:317\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             filename = response.read()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\io\\file.py:577\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    568\u001b[39m     warnings.warn(\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyogrio\\geopandas.py:371\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    369\u001b[39m geometry = shapely.from_wkb(geometry, on_invalid=on_invalid)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcrs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\geodataframe.py:240\u001b[39m, in \u001b[36mGeoDataFrame.__init__\u001b[39m\u001b[34m(self, data, geometry, crs, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(geometry, pd.Series) \u001b[38;5;129;01mand\u001b[39;00m geometry.name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    235\u001b[39m     ):\n\u001b[32m    236\u001b[39m         \u001b[38;5;66;03m# __init__ always creates geometry col named \"geometry\"\u001b[39;00m\n\u001b[32m    237\u001b[39m         \u001b[38;5;66;03m# rename as `set_geometry` respects the given series name\u001b[39;00m\n\u001b[32m    238\u001b[39m         geometry = geometry.rename(\u001b[33m\"\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAssigning CRS to a GeoDataFrame without a geometry column is not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupported. Supply geometry using the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword argument, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor by providing a DataFrame with column name \u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    247\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\geodataframe.py:459\u001b[39m, in \u001b[36mGeoDataFrame.set_geometry\u001b[39m\u001b[34m(self, col, drop, inplace, crs)\u001b[39m\n\u001b[32m    456\u001b[39m     crs = \u001b[38;5;28mgetattr\u001b[39m(level, \u001b[33m\"\u001b[39m\u001b[33mcrs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# Check that we are using a listlike of geometries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m level = \u001b[43m_ensure_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# ensure_geometry only sets crs on level if it has crs==None\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level, GeoSeries):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\geodataframe.py:71\u001b[39m, in \u001b[36m_ensure_geometry\u001b[39m\u001b[34m(data, crs)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GeoSeries(out, index=data.index, name=data.name)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     out = \u001b[43mfrom_shapely\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\array.py:192\u001b[39m, in \u001b[36mfrom_shapely\u001b[39m\u001b[34m(data, crs)\u001b[39m\n\u001b[32m    189\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput must be valid geometry objects: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeom\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m     arr = np.array(out, dtype=\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeometryArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\array.py:348\u001b[39m, in \u001b[36mGeometryArray.__init__\u001b[39m\u001b[34m(self, data, crs)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m._data = data\n\u001b[32m    347\u001b[39m \u001b[38;5;28mself\u001b[39m._crs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcrs\u001b[49m = crs\n\u001b[32m    349\u001b[39m \u001b[38;5;28mself\u001b[39m._sindex = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\geopandas\\array.py:401\u001b[39m, in \u001b[36mGeometryArray.crs\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m HAS_PYPROJ:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CRS\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28mself\u001b[39m._crs = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mCRS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyproj\\crs\\crs.py:503\u001b[39m, in \u001b[36mCRS.from_user_input\u001b[39m\u001b[34m(cls, value, **kwargs)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyproj\\crs\\crs.py:350\u001b[39m, in \u001b[36mCRS.__init__\u001b[39m\u001b[34m(self, projparams, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m._local.crs = projparams\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28mself\u001b[39m._local.crs = \u001b[43m_CRS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olive\\anaconda3\\envs\\base_data_analysis\\Lib\\site-packages\\pyproj\\_crs.pyx:2364\u001b[39m, in \u001b[36mpyproj._crs._CRS.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCRSError\u001b[39m: Invalid projection: EPSG:4326: (Internal Proj Error: proj_create: no database context specified)"
     ]
    }
   ],
   "source": [
    "# city boundaries of washington\n",
    "washington_boundary = gpd.read_file(config['raw_data_paths']['washington_outline'])\n",
    "washington_boundary = washington_boundary.drop(columns=[\n",
    "    'AREAMILES', 'OBJECTID', 'STATE_CITY', 'CAPITAL', 'WEB_URL', 'GLOBALID', 'CREATOR', 'CREATED', 'EDITOR','EDITED', 'SHAPEAREA', 'SHAPELEN'\n",
    "    ])\n",
    "washington_boundary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57736287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized rides data \n",
    "df = pd.read_parquet(config['processed_data_paths']['rides_init'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b00f7",
   "metadata": {},
   "source": [
    "# Features\n",
    "- is_within_city: ride is within the city boundaries, i.e. Start and End are both within the boundaries\n",
    "- is_holiday: Day of ride is a holiday\n",
    "- ride_duration: ride duration in s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9910bb1",
   "metadata": {},
   "source": [
    "## Ride within boundaries of city center\n",
    "- Are the rides within the city center service area? The activities outside might not be that interesting\n",
    "- Problem: no map of the full service boundaries is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4329e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE gpd thinks in longitude and latitude, not the other way around\n",
    "def check_if_ride_within_city_boundaries(df, city_boundary) -> pd.Series:\n",
    "    geometry = [LineString([(lng1, lat1), (lng2, lat2)]) for lat1, lng1, lat2, lng2 in zip(df.start_lat, df.start_lng, df.end_lat, df.end_lng)]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    rides_with_city_info = gpd.sjoin(gdf, city_boundary, how=\"left\", predicate=\"intersects\", lsuffix='point', rsuffix='city')\n",
    "    return rides_with_city_info.index_city.notna()\n",
    "\n",
    "def check_if_start_within_city_boundaries(df, city_boundary) -> pd.Series:\n",
    "    geometry = [Point((lng1, lat1)) for lat1, lng1 in zip(df.start_lat, df.start_lng)]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    rides_with_city_info = gpd.sjoin(gdf, city_boundary, how=\"left\", predicate=\"within\", lsuffix='point', rsuffix='city')\n",
    "    return rides_with_city_info.index_city.notna()\n",
    "\n",
    "def check_if_end_within_city_boundaries(df, city_boundary) -> pd.Series:\n",
    "    geometry = [Point(lng2, lat2) for lat2, lng2 in zip(df.end_lat, df.end_lng)]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    rides_with_city_info = gpd.sjoin(gdf, city_boundary, how=\"left\", predicate=\"within\", lsuffix='point', rsuffix='city')\n",
    "    return rides_with_city_info.index_city.notna()\n",
    "\n",
    "df['start_is_within_city']  = check_if_start_within_city_boundaries(df[['start_lat', 'start_lng', 'end_lat', 'end_lng']], washington_boundary)\n",
    "df['end_is_within_city']    = check_if_end_within_city_boundaries(df[['start_lat', 'start_lng', 'end_lat', 'end_lng']], washington_boundary)\n",
    "df['ride_is_within_city']   = df['start_is_within_city'] & df['end_is_within_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e33b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO why this here? why is a new index created? \n",
    "# It does not matter, so the new index can be taken\n",
    "if 'index' in df.columns: #hasattr(df, 'index'):\n",
    "    df = df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move to other ipynb\n",
    "mask_2023 = df.started_at.dt.year >= 2023\n",
    "\n",
    "print('rides that start outside the city limits: ', sum(~df.loc[mask_2023, 'start_is_within_city']) / df[mask_2023].shape[0])\n",
    "print('rides that end outside the city limits: ',   sum(~df.loc[mask_2023, 'end_is_within_city']) / df[mask_2023].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot the rides that are outside the city boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bda2f",
   "metadata": {},
   "source": [
    "## Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf15e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_holiday'] = df.started_at.dt.date.apply(lambda x: x in US_HOLIDAYS.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_4thjuly = df.started_at.dt.date == datetime.date(2023, 7, 4)\n",
    "# df[mask_4thjuly]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ba5b6",
   "metadata": {},
   "source": [
    "## Ride Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ride_duration'] = df.ended_at - df.started_at\n",
    "df['ride_duration'] = df['ride_duration'].apply(lambda x: x.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea46932",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "- separate redundant information and create separate tables\n",
    "  - station table: station_id, lat, lng (lat, log as mean values), start, end\n",
    "- transform data\n",
    "  - rideable_type -> 0,1,2,... OR classic: True, False\n",
    "  - member_casual -> 0,1,2,...\n",
    "- data integrity\n",
    "  - lat, lng in Washington bbox\n",
    "  - if start / end - lat / lng == null, then remove\n",
    "- reduce data type to reasonable precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c751716",
   "metadata": {},
   "source": [
    "## Ride duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862f604",
   "metadata": {},
   "source": [
    "### Drop rides that are too short to be true\n",
    "Only keep rides that lasted longer than 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae82688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride duration below 30s\n",
    "rides_too_short_to_be_true = df.ride_duration < 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4992ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_too_short_to_be_true.sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3af645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~rides_too_short_to_be_true]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ea6f",
   "metadata": {},
   "source": [
    "### Tag rides that are too long to be true\n",
    "Ride durations of > 1d are exceptional and will be tagged for the following analysis.\n",
    "Reasons for long ride duration\n",
    "- Rides have rented a bike for multiple days on purpose.\n",
    "- Riders must have made an error when handing back the bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57635dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag rides with ride duration > 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5187123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF for outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f900cb3",
   "metadata": {},
   "source": [
    "## Latitude and Longitude\n",
    "\n",
    "Check if cooordinates in bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# service area bbox\n",
    "min_lng, min_lat = -77.454987, 38.701588\n",
    "max_lng, max_lat = -76.784821, 39.168400\n",
    "service_area_bbox = (min_lng, min_lat, max_lng, max_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lat_lng_outside_bbox = ((df.start_lng < min_lng) | (df.start_lng > max_lng) | (df.end_lng < min_lng) | (df.end_lng > max_lng) |\n",
    "(df.start_lat < min_lat) | (df.start_lat > max_lat) | (df.end_lat < min_lat) | (df.end_lat > max_lat))\n",
    "\n",
    "mask_lat_lng_outside_bbox.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~mask_lat_lng_outside_bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62cfd4",
   "metadata": {},
   "source": [
    "## Stations\n",
    "\n",
    "Clean station data such that theres only one lat, lng per station, in order to **???**\n",
    "- simplify the data: Replace the bike location with station location if bike is returned at a station\n",
    "\n",
    "1. Get all station data from rides table\n",
    "2. Build station table\n",
    "3. Clean main data set with station table\n",
    "\n",
    "Questions\n",
    "- Are stations moved?\n",
    "\n",
    "Problems:\n",
    "- stations have many locations that vary strongly\n",
    "  - Can the station locations be determined by the available data set?\n",
    "- station_id, station_name have more unique values than station_id\n",
    "  - What does that mean?\n",
    "- Are stations moved? What happens in the data set?\n",
    "- Errors in the bike location up to 10m\n",
    "\n",
    "Assumptions\n",
    "- a station's location is determined by where most of the bikes are put.\n",
    "- all ride data associated with \n",
    "\n",
    "Steps in order to build station table\n",
    "1. clean station names\n",
    "2. set precision for lat and long\n",
    "3. remove station coordinate outliers\n",
    "4. calculate averages of location coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71a06a",
   "metadata": {},
   "source": [
    "### Clean Station Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean station names\n",
    "# TODO is there a way to make it more efficient? \n",
    "df.start_station_name = df.start_station_name.str.strip()\n",
    "df.end_station_name = df.end_station_name.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48c027",
   "metadata": {},
   "source": [
    "### Some overall stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_start = ~df.start_station_id.isna()\n",
    "mask_end = ~df.end_station_id.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c9cc9",
   "metadata": {},
   "source": [
    "### Use docked bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd21b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_docked_bikes = df.rideable_type=='docked_bike'\n",
    "\n",
    "stations_docked_bikes = [None, None]\n",
    "stations_docked_bikes[0] = get_station_data(df, mask_docked_bikes & mask_start, ['ended_at', 'start_station_name', 'start_station_id', 'start_lat', 'start_lng'], NDEC_LAT_LNG, False)\n",
    "stations_docked_bikes[1] = get_station_data(df, mask_docked_bikes & mask_end, ['ended_at', 'end_station_name', 'end_station_id', 'end_lat', 'end_lng'], NDEC_LAT_LNG, False)\n",
    "\n",
    "stations_docked_bikes = pd.concat(stations_docked_bikes, axis=0)\n",
    "\n",
    "stations_docked_bikes = stations_docked_bikes.groupby(['station_name', 'station_id']).agg({\n",
    "    'lat' : ['median', 'mean', 'std', 'count'],\n",
    "    'lng' : ['median', 'mean', 'std', 'count'],\n",
    "    'date': ['min', 'max']\n",
    "    })\n",
    "\n",
    "stations_docked_bikes.columns = ['_'.join(col) for col in stations_docked_bikes.columns]\n",
    "stations_docked_bikes = stations_docked_bikes.reset_index()\n",
    "stations_docked_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_serviced = pd.concat(\n",
    "    [df[['ended_at', 'start_station_id', 'start_station_name']].rename(columns=lambda x: x.replace('start_','')), \n",
    "     df[['ended_at', 'end_station_id', 'end_station_name']].rename(columns=lambda x: x.replace('end_',''))],\n",
    "    axis=0\n",
    "    )\n",
    "stations_serviced.ended_at = stations_serviced.ended_at.dt.date\n",
    "stations_serviced = stations_serviced.groupby(['station_id', 'station_name']).agg({'ended_at':['min', 'max']})\n",
    "stations_serviced.columns = ['_'.join(col) for col in stations_serviced.columns]\n",
    "stations_serviced = stations_serviced.reset_index()\n",
    "stations_serviced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stations_services = pd.merge(stations_docked_bikes, stations_serviced, on=['station_id', 'station_name'], how='left')\n",
    "check_stations_services['check'] = check_stations_services.date_max - check_stations_services.ended_at_max\n",
    "check_stations_services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get station where data is too old to be true\n",
    "redo_stations = check_stations_services.loc[check_stations_services.check < pd.Timedelta(days=-120), ['station_id', 'station_name']]\n",
    "redo_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022576ae",
   "metadata": {},
   "source": [
    "- locations are good\n",
    "- not all stations are included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c48cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move downwards : stations_names_ids not yet defined!\n",
    "stations_names_ids = pd.concat([\n",
    "    df[['start_station_id', 'start_station_name']].rename(columns=lambda x: x.replace('start_','')),\n",
    "    df[['end_station_id', 'end_station_name']].rename(columns=lambda x: x.replace('end_',''))\n",
    "    ], axis=0).drop_duplicates()\n",
    "stations_names_ids = stations_names_ids[stations_names_ids.station_id.notna() & stations_names_ids.station_name.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stations = pd.merge(stations_names_ids, stations_docked_bikes, on=['station_id', 'station_name'], how='outer')\n",
    "missing_stations = missing_stations[missing_stations.lat_mean.isna() | missing_stations.lng_mean.isna()]\n",
    "missing_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stations[missing_stations.duplicated(subset=['station_id', 'station_name'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13a0f2",
   "metadata": {},
   "source": [
    "### Get station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all station data\n",
    "start_stations = get_station_data(df, mask_start, ['ended_at', 'start_station_name', 'start_station_id', 'start_lat', 'start_lng'], dec=NDEC_LAT_LNG, drop_dups=False)\n",
    "end_stations = get_station_data(df, mask_end, ['ended_at', 'end_station_name','end_station_id', 'end_lat', 'end_lng'], dec=NDEC_LAT_LNG, drop_dups=False)\n",
    "stations = pd.concat(\n",
    "    [start_stations, end_stations],\n",
    "    axis = 0\n",
    "    )\n",
    "\n",
    "# group by \n",
    "stations = stations.groupby(['station_name', 'station_id', 'lat', 'lng']).agg({'date':['count', 'min', 'max']})\n",
    "stations.columns = ['_'.join(col) for col in stations.columns]\n",
    "stations.rename(columns={'date_count':'count'})\n",
    "stations = stations.reset_index()\n",
    "\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb518a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to missing station data\n",
    "mask_missing_stations = (\n",
    "    (stations.station_id.isin(missing_stations.station_id) & stations.station_name.isin(missing_stations.station_name)) |\n",
    "    (stations.station_id.isin(redo_stations.station_id) & stations.station_name.isin(redo_stations.station_name))\n",
    "    )\n",
    "print(mask_missing_stations.sum())\n",
    "stations_calc = stations[mask_missing_stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e096a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_calc.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b735f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample on map\n",
    "n_ids = 10\n",
    "n_stations_per_id = 1000\n",
    "\n",
    "# ids = station_ids.sample(n_ids)\n",
    "ids = list(stations.station_id.value_counts().sort_values(ascending=False).head(n_ids).index)\n",
    "\n",
    "stations_samples = []\n",
    "for i in ids:\n",
    "    stations_sample = stations[stations.station_id == i]\n",
    "    if stations_sample.shape[0] > n_stations_per_id:\n",
    "        stations_sample = stations_sample.sample(n_stations_per_id)\n",
    "    stations_samples.append(stations_sample)\n",
    "stations_samples = pd.concat(stations_samples, axis=0)\n",
    "stations_samples.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959fac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "sns.scatterplot(stations_samples, x='lat', y='lng', hue=stations_samples.station_id.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fa788",
   "metadata": {},
   "source": [
    "### Clustering of Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO needs some data cleaning first!\n",
    "# nr_stations = len(set(zip(stations['station_id'], stations['station_name'])))\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # write a cython function?\n",
    "# # use haversine instead, it's a good approximation\n",
    "# # geodesic_m = lambda x, y:  geodesic(x, y).m\n",
    "\n",
    "# X = np.radians(np.array(list(zip(stations.lat, stations.lng))))\n",
    "\n",
    "# nbrs = NearestNeighbors(n_neighbors=nr_stations, metric='haversine', algorithm='ball_tree', n_jobs=-1).fit(X)\n",
    "# distances, indcs = nbrs.kneighbors(X)\n",
    "# distances *= EARTH_RADIUS_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_moved = np.apply_along_axis(lambda d: (d > 10).any(), axis=0, arr=distances)\n",
    "# print(stations_moved.shape)\n",
    "# stations_moved.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1cd16",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data for functions below\n",
    "# ids = np.random.choice(stations.station_id.unique(), size=10, replace=False)\n",
    "# stations_test_set = stations[stations.station_id.apply(lambda id: id in ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import math\n",
    "\n",
    "def dist_haversine(u, v):\n",
    "    # Differences in coordinates\n",
    "    dlat = v[0] - u[0]\n",
    "    dlon = v[1] - u[1]\n",
    "\n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(u[0]) * math.cos(v[0]) * math.sin(dlon / 2)**2\n",
    "    return 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "def remove_outliers_lof(group, lat_lng_std_tol=1-6, dist_tol=1):\n",
    "    # data prep\n",
    "    v = np.array(group[['lat', 'lng']]).astype(np.float32)\n",
    "    # lat_mean, lat_std = np.mean(group[['lat']]), np.std(group[['lat']])\n",
    "    # lng_mean, lng_std = np.mean(group[['lng']]), np.std(group[['lng']])\n",
    "    # if (lat_std<tol) & (lng_std<tol):\n",
    "    #     return group\n",
    "    if len(v) < 5:\n",
    "        return group\n",
    "    v = np.radians(v)\n",
    "    u = v\n",
    "    # calc distances\n",
    "    distances = cdist(u, v, metric=dist_haversine)\n",
    "    # apply dist tolerance\n",
    "    distances[distances * EARTH_RADIUS_M < dist_tol] = 0.\n",
    "    # find outliers\n",
    "    lof = LocalOutlierFactor(metric='precomputed', n_jobs=-1)\n",
    "    pred = lof.fit_predict(distances)\n",
    "    # mark outliers\n",
    "    return group.iloc[np.where(pred>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     stations_wo_outliers = pd.DataFrame()\n",
    "#     stations_wo_outliers = pd.read_parquet(config['processed_data_paths']['stations_wo_outliers'])\n",
    "# except:\n",
    "#     stations_wo_outliers = stations.groupby(['station_id', 'station_name']).apply(\n",
    "#         remove_outliers_lof, lat_lng_std_tol=1e-4, dist_tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_calc[['station_id']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stations_calc.groupby(['station_id', 'station_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11378782",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_wo_outliers = stations_calc.groupby(['station_id', 'station_name']).apply(\n",
    "    remove_outliers_lof, \n",
    "    lat_lng_std_tol=1e-4, \n",
    "    dist_tol=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f46c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stations_calc.shape)\n",
    "print(stations_wo_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2568147",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_wo_outliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce981a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796874f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO stations_wo_outliers has station_id, station_name in index and in columns - fix this\n",
    "stations_wo_outliers = stations_wo_outliers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4dfb9",
   "metadata": {},
   "source": [
    "### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7afa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_agg = stations_wo_outliers.groupby(['station_id', 'station_name']).agg({\n",
    "    'date_min' : 'min',\n",
    "    'date_max' : 'max',\n",
    "    'lat' : ['median', 'mean', 'std', 'max', 'min'],\n",
    "    'lng' : ['median', 'mean', 'std', 'max', 'min']\n",
    "})\n",
    "stations_agg.columns = ['_'.join(col) for col in stations_agg.columns]\n",
    "stations_agg = stations_agg.reset_index()\n",
    "stations_agg = stations_agg.rename(columns={'date_min_min':'start_service', 'date_max_max':'end_service'})\n",
    "stations_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383eff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_agg = stations_wo_outliers.groupby(['station_id', 'station_name']).agg({\n",
    "#     'date_min' : 'min',\n",
    "#     'date_max' : 'max',\n",
    "#     'lat' : ['mean', 'std', 'max', 'min'],\n",
    "#     'lng' : ['mean', 'std', 'max', 'min']\n",
    "# })\n",
    "\n",
    "# stations_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b1f3b",
   "metadata": {},
   "source": [
    "### Analyse station table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_agg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where does the lat, lng differ strongly?\n",
    "# stations_agg[(stations_agg[('lat','std')]>1e-3) | (stations_agg[('lng','std')]>1e-3)].sort_values(by=('lat','std'), ascending=False)\n",
    "stations_agg[(stations_agg[('lat_std')]>1e-3) | (stations_agg[('lng_std')]>1e-3)].sort_values(by=('lat_std'), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a854042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate diagonals of station bounding boxes\n",
    "# stations bounding box:\n",
    "def geodesic_m(lat1, lng1, lat2, lng2):\n",
    "    return [geodesic(gPoint(x1, y1),gPoint(x2, y2)).m for x1, y1, x2, y2 in zip(lat1, lng1, lat2, lng2)]\n",
    "\n",
    "# stations_agg['diagonal_bbox'] = geodesic_m(stations_agg[('lat', 'min')], stations_agg[('lng', 'min')], stations_agg[('lat', 'max')], stations_agg[('lng', 'max')])\n",
    "stations_agg['diagonal_bbox'] = geodesic_m(stations_agg[('lat_min')], stations_agg[('lng_min')], stations_agg[('lat_max')], stations_agg[('lng_max')])\n",
    "stations_agg.sort_values(by='diagonal_bbox', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1122583",
   "metadata": {},
   "source": [
    "* bounding boxes are up to 20km large!\n",
    "* there are large outliers in the feckin data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d3027",
   "metadata": {},
   "source": [
    "### Build stations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_docked_bikes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f06975",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f027e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_docked_bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_cols = ['station_name', 'station_id', 'lat_median','lng_median']\n",
    "stations = pd.concat([stations_docked_bikes[station_cols], stations_agg[station_cols]], axis=0)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_redo_1 = stations[stations.duplicated(['station_name', 'station_id'], keep='first')].set_index(['station_id', 'station_name'])\n",
    "stations_redo_2 = stations[stations.duplicated(['station_name', 'station_id'], keep='last')].set_index(['station_id', 'station_name'])\n",
    "\n",
    "diff = pd.DataFrame(data=[\n",
    "    abs(stations_redo_1.lat_median - stations_redo_2.lat_median),\n",
    "    abs(stations_redo_1.lng_median - stations_redo_2.lng_median) \n",
    "]).T\n",
    "\n",
    "TOL = 1e-3\n",
    "diff[(diff.lat_median>TOL) | (diff.lng_median>TOL)].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a40c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations.drop_duplicates(subset=['station_id', 'station_name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5325392",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_ = stations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa65ef",
   "metadata": {},
   "source": [
    "### Merge Meta Data: Count, Service Dates\n",
    "\n",
    "- Count: How often was the feckin station targeted in the whole period?\n",
    "- Service Dates: When was the first and last service date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all station data\n",
    "start_stations = get_station_data(df, mask_start, ['ended_at', 'start_station_name', 'start_station_id'], dec=NDEC_LAT_LNG, drop_dups=False)\n",
    "end_stations = get_station_data(df, mask_end, ['ended_at', 'end_station_name','end_station_id'], dec=NDEC_LAT_LNG, drop_dups=False)\n",
    "stations_service_dates = pd.concat(\n",
    "    [start_stations, end_stations],\n",
    "    axis = 0\n",
    "    )\n",
    "\n",
    "# group by \n",
    "stations_service_dates = stations_service_dates.groupby(['station_name', 'station_id']).agg({'date':['count', 'min', 'max']})\n",
    "stations_service_dates.columns = ['_'.join(col) for col in stations_service_dates.columns]\n",
    "stations_service_dates = stations_service_dates.rename(columns={'date_count':'total_count_start_end'})\n",
    "stations_service_dates = stations_service_dates.rename(columns={'date_min' : 'date_start', 'date_max' : 'date_end'})\n",
    "stations_service_dates = stations_service_dates.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_service_dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_service_dates.loc[stations_service_dates.date_end>=pd.Timestamp('2023.12.31').date(), 'date_end'] = np.nan\n",
    "stations_service_dates.sort_values(by='total_count_start_end', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.merge(stations, stations_service_dates, on=['station_id', 'station_name'], how='left')\n",
    "stations.index.name = 'station_id'\n",
    "stations = stations.rename(columns={'station_id':'station_id_old'})\n",
    "stations.index = stations.index.astype('uint32[pyarrow]')  \n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fad765",
   "metadata": {},
   "source": [
    "### Add new station_id column to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge new station index to dataframe\n",
    "df = pd.merge(df,\n",
    "         left_on=['start_station_id', 'start_station_name'],\n",
    "         right=stations[['station_id_old', 'station_name']].reset_index().set_index(['station_id_old', 'station_name']),\n",
    "         right_index=True,\n",
    "         how='left').drop(columns=['start_station_id', 'start_station_name'])\n",
    "df = df.rename(columns={'start_station_id':'start_station_id_old'})\n",
    "df = df.rename(columns={'station_id':'start_station_id'})\n",
    "\n",
    "df = pd.merge(df,\n",
    "         left_on=['end_station_id', 'end_station_name'],\n",
    "         right=stations[['station_id_old', 'station_name']].reset_index().set_index(['station_id_old', 'station_name']),\n",
    "         right_index=True,\n",
    "         how='left').drop(columns=['end_station_id', 'end_station_name'])\n",
    "df = df.rename(columns={'end_station_id':'end_station_id_old'})\n",
    "df = df.rename(columns={'station_id':'end_station_id'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead21e3",
   "metadata": {},
   "source": [
    "## Bike Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227b23d",
   "metadata": {},
   "source": [
    "### Docked Bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docked bikes\n",
    "docked_bikes = df.loc[df.rideable_type == 'docked_bike']\n",
    "print(docked_bikes.shape)\n",
    "docked_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docked_bikes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568aef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if docked bikes are ever put outside a station\n",
    "print(docked_bikes.start_station_id.isna().sum())\n",
    "print(docked_bikes.end_station_id.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rideable_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cc63c",
   "metadata": {},
   "source": [
    "docked_bikes can only be ridden from station to station -> classic bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3788b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.rideable_type == 'docked_bikes'].rideable_type = 'classic_bike'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76cced8",
   "metadata": {},
   "source": [
    "### E-Bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_electric_bikes = df.loc[df.rideable_type == 'electric_bike']\n",
    "df_electric_bikes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba479e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_electric_bikes.start_station_id.isna().sum(), df_electric_bikes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383767e",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ef111",
   "metadata": {},
   "source": [
    "## Create Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8959e79",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26279e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(config['processed_data_paths']['rides'])\n",
    "stations.to_parquet(config['processed_data_paths']['stations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf97dc",
   "metadata": {},
   "source": [
    "# Conclusions from Data Cleaning\n",
    "\n",
    "- Bike location measurements has to be improved: They spread far around a station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e5036",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
